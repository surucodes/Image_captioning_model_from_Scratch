# -*- coding: utf-8 -*-
"""Image_captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bnGjU0acSFRjuH9qvwyfS3oSfNDeMH3m
"""

import tensorflow as tf
import string
import numpy as np
import os
from pickle import dump, load
import matplotlib.pyplot as plt
from tensorflow.keras.applications.xception import Xception, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import add, Input, Dense, LSTM, Embedding, Dropout
from tensorflow.keras.models import Model, load_model
from tqdm import tqdm_notebook as tqdm
import time
from PIL import Image
tqdm().pandas()

from tensorflow.keras.utils import get_file

#load text file into the memory
def load_doc(file_name):
    file = open(file_name, 'r')
    text = file.read()
    file.close
    return text

'''we convert images into features using the xception model. We give image feature and Caption sequence
as input features and captions are predicted sequence by sequence using lstm etc '''
''' we have the same single image 5 times and 5 different captions for the same image '''
def all_img_captions(filename):
     file = load_doc(filename)
     captions = file.split('\n')
     descriptions = {}
     for caption in captions[:-1]:
        img,caption = caption.split('\t')
        if img[:-2] not in descriptions:
             descriptions[img[:-2]] = [caption]
        else:
            descriptions[img[:-2]].append(caption)
     return descriptions

def cleaning_text(captions):
    table = str.maketrans('', '',string.punctuation)
    for img, caps in captions.items():
        for i,img_caption in enumerate(caps):
            img_caption=img_caption.replace("-","")
            desc = img_caption.split()
            desc = [word.lower() for word in desc]
            desc = [word.translate(table) for word in desc]
            desc = [word for word in desc if len(word) > 1]
            desc = [word for word in desc if(word.isalpha())]

            img_caption = ' '.join(desc)
            captions[img][i] = img_caption
    return captions

def text_vocabulary(descriptions):
    vocab = set()
    for key in descriptions.keys():
        [vocab.update(d.split()) for d in descriptions[key]]
    return vocab

def save_descriptions(descriptions, filename):
    lines = list()
    for key, desc_list in descriptions.items():
        for desc in desc_list:
            lines.append(key + '\t' + desc)
    data = "\n".join(lines)
    file = open(filename,"w")
    file.write(data)
    file.close()

from google.colab import drive
import os
import zipfile

# Mount Google Drive
drive.mount('/content/drive')

# Define paths to the zip files in your Google Drive
drive_flickr_dataset_zip = "/content/drive/MyDrive/Flickr8k_Dataset.zip"
drive_flickr_text_zip = "/content/drive/MyDrive/Flickr8k_text.zip"

# Create directories to extract the files
!mkdir -p /content/Flickr8k_text
!mkdir -p /content/Flicker8k_Dataset

# Extract the zip files
with zipfile.ZipFile(drive_flickr_dataset_zip, 'r') as zip_ref:
    zip_ref.extractall('/content/Flicker8k_Dataset')

with zipfile.ZipFile(drive_flickr_text_zip, 'r') as zip_ref:
    zip_ref.extractall('/content/Flickr8k_text')

# Define the paths as in your original code
dataset_text = "/content/Flickr8k_text"
dataset_images = "/content/Flicker8k_Dataset/Flicker8k_Dataset"
filename = dataset_text + "/Flickr8k.token.txt"

# Verify the files are loaded correctly
print(f"Text dataset path: {dataset_text}")
print(f"Images dataset path: {dataset_images}")
print(f"Token file path: {filename}")

# Check if the token file exists
if os.path.exists(filename):
    # Read a few lines from the token file to verify
    with open(filename, 'r') as f:
        print("First few lines of the token file:")
        for i, line in enumerate(f):
            if i < 5:  # Print first 5 lines
                print(line.strip())
            else:
                break
else:
    print("Token file not found. Check the path and extraction.")

print(dataset_images)  # Should be your image folder path
print(os.listdir(dataset_text)[:5])  # Sample of your images



descriptions = all_img_captions(filename)
print("Length of descriptions = ",len(descriptions))
# descriptions

clean_descriptions = cleaning_text(descriptions)
vocabulary = text_vocabulary(clean_descriptions)
print("Length of vocabulary =", len(vocabulary))
# vocabulary

save_descriptions(clean_descriptions,"descriptions.txt")

save_path = "/content/drive/MyDrive/ImageCaptioning/descriptions.txt"
save_descriptions(clean_descriptions, save_path)



print(clean_descriptions)

def download_with_retry(url,filename,max_retries = 3):
    for attempt in range(max_retries):
        try:
            return get_file(filename,url)
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            print(f"Download attempt failed")
            time.sleep(3)

weights_url = "https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5"
weights_path = download_with_retry(weights_url, 'xception_weights.h5')

model = Xception(include_top=False, pooling= "avg",weights=weights_path)

'''
os.path.splitext(img) splits the filename into two parts:
The name without the extension
The extension (including the dot)
Example:
img = "dog.JPG"
os.path.splitext(img)  # returns ('dog', '.JPG')
[1] takes the second element, i.e., the extension: ".JPG" and .lower() makes it lowercase: ".jpg"
This helps in checking file types case-insensitively. '''

'''Let‚Äôs say your image shape is:
(299, 299, 3)  # One image of size 299x299 with 3 color channels
But deep learning models (like Xception) expect batches of images, even if it's just 1 image.
So np.expand_dims(image, axis=0) adds a new dimension:
(1, 299, 299, 3)  # batch size of 1
Think of it like putting your single image inside a "batch box" üì¶
This makes it compatible with Xception model '''
def extract_features(directory):
    features= {}
    valid_images= ['.jpg','.jpeg','.png']
    for img in tqdm(os.listdir(directory)):
        ext= os.path.splitext(img)[1].lower()
        if ext not in valid_images:
            continue
        filename = os.path.join(directory, img)
        image = Image.open(filename)
        image = image.resize((299,299))
        image = np.expand_dims(image,axis=0)
        image = image/127.5
        image = image - 1.0
        feature = model.predict(image)
        features[img]= feature
    return features
'''
What Is the Output of model.predict(image)?
It outputs a NumPy array like:
array([[ 0.123, -0.948, ..., 1.203 ]])  # shape: (1, 2048)
This is the feature vector for that image ‚Äî a compact representation of its abstract visual features (edges, shapes, textures, etc., as we discussed earlier).

 Why Do We Do This for Every Image?
We're building a dictionary of image features that looks like this:
features = {
    "image1.jpg": array([[... 2048 features ...]]), shape(of each feature vector of individual images in the dataset): (1, 2048)
    "image2.jpg": array([[... 2048 features ...]]),
    ...
}
'''

features = extract_features(dataset_images)

dump(features, open("features.p","wb"))

print(f"Number of features extracted: {len(features)}")
print("Sample keys:", list(features.keys())[:5])

# import pickle
# with open('flickr8k_features.pkl', 'wb') as f:
#     pickle.dump(features, f)
# print("Features saved!")
# with open('flickr8k_features.pkl', 'rb') as f:
#     loaded_features = pickle.load(f)
# print(f"Loaded {len(loaded_features)} features")

# from google.colab import files
# files.download("features.p")

import os

folder_path = '/content/drive/MyDrive/ImageCaptioning'
os.makedirs(folder_path, exist_ok=True)

# Save the file in that folder
dump(features, open(os.path.join(folder_path, 'features.p'), 'wb'))

features = load(open("features.p","rb"))

import os
from pickle import load
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define the folder and file path
folder_path = '/content/drive/MyDrive/ImageCaptioning'
file_path = os.path.join(folder_path, 'features.p')

# Load the features from the file
features = load(open(file_path, 'rb'))
print(f"Loaded {len(features)} features from {file_path}")

# Optional: Verify a few entries
print("Sample feature keys:", list(features.keys())[:5])

def load_photos(filename):
  file=load_doc(filename)
  photos = file.split("\n")[:-1]
  photos_present = [photo for photo in photos if os.path.exists(os.path.join(dataset_images,photo))]
  return photos_present

def load_clean_descriptions(filename , photos):
  file = load_doc(filename)
  descriptions = {}
  for line in file.split("\n"):
    words = line.split()
    if len(words) < 1:
      continue

    image,image_caption = words[0] , words[1:]
    if image in photos :
      if image not in descriptions:
        descriptions[image] = []
      desc = '<start>' + " ".join(image_caption) + ' <end>'
      descriptions[image].append(desc)
  return descriptions

def load_features(photos):
  all_features = load(open(file_path, 'rb'))
  features = {k:all_features[k] for k in photos}
  print(features)
  return features

filename = dataset_text + "/" + "Flickr_8k.trainImages.txt"
train_imgs = load_photos(filename)
train_descriptions = load_clean_descriptions(save_path,train_imgs)
train_features = load_features(train_imgs)

print("Train descriptions length:", len(train_descriptions))
print("Train features length:", len(train_features))
print("Sample description:", list(train_descriptions.items())[0])
print("Sample feature shape:", train_features[list(train_features.keys())[0]].shape)

key = list(train_descriptions.keys())[0]
desc_list = train_descriptions[key]
feature = train_features[key][0]
input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, desc_list, feature)
print("Input image shape:", input_image.shape)
print("Input sequence shape:", input_sequence.shape)
print("Output word shape:", output_word.shape)

dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)
for x, y in dataset.take(1):
    print(x.keys(), x['input_1'].shape, x['input_2'].shape)
    print(y.shape)

import os
print(os.path.exists(filename))  # Should print True
with open(filename, 'r') as f:
    print(f.read().splitlines()[:5])  # Print first 5 lines

print(train_imgs)

def dict_to_list(descriptions):
  all_desc = []
  for key in descriptions.keys():
    [all_desc.append(d) for d in descriptions[key]]
  return all_desc

def create_tokenizer(descriptions):
  desc_list = dict_to_list(descriptions)
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(desc_list)
  return tokenizer
'''By default, it:
Lowercases all text
Filters out punctuation
Assigns a unique integer to each word'''

tokenizer = create_tokenizer(train_descriptions)

dump(tokenizer, open("tokenizer.p","wb"))

import shutil

# Same folder as features.p
tokenizer_path = "/content/drive/MyDrive/ImageCaptioning/tokenizer.p"

# Save tokenizer.p from Colab's local directory to Drive
shutil.copy("tokenizer.p", tokenizer_path)

vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)

print(train_descriptions)

def max_length(descriptions):
  desc_list = dict_to_list(descriptions)
  return max(len(d.split()) for d in desc_list)

max_length = max_length(train_descriptions)
print(max_length)

'''
create_sequences:
Takes a list of captions for one image and its feature vector.
Generates all possible ‚Äúpartial caption ‚Üí next word‚Äù pairs for training.
Outputs NumPy arrays of image features, padded sequences, and one-hot encoded next words.'''
'''This function is used for training an image captioning model.

It takes a caption and the corresponding image feature vector and generates pairs like:

"Image features + partial caption ‚Üí next word"
'''
def create_sequences(tokenizer, max_length, desc_list, feature):
    X1, X2, y = list(), list(), list()
    # walk through each description for the image
    for desc in desc_list:
        # encode the sequence
        seq = tokenizer.texts_to_sequences([desc])[0]
        # split one sequence into multiple X,y pairs
        for i in range(1, len(seq)):
            # split into input and output pair
            in_seq, out_seq = seq[:i], seq[i]
            # pad input sequence
            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
            # encode output sequence
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            # store
            X1.append(feature)
            X2.append(in_seq)
            y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

# This function is used for training an image captioning model.
# It takes a caption and the corresponding image feature vector and generates pairs like:

"Image features + partial caption ‚Üí next word"
# Inside the data_generator function
def data_generator(descriptions , features , tokenizer , max_length):
  def generator():
    while True:
      for key, description_list in descriptions.items():
        feature = features[key][0]
        input_image, input_sequence, output_word = create_sequences(tokenizer,max_length,description_list,feature)
        for i in range(len(input_image)):
          yield{'input_1' : input_image[i], 'input_2': input_sequence[i]} , output_word[i]

  output_signature = (
        {
            'input_1': tf.TensorSpec(shape=(2048,), dtype = (tf.float32)),
            'input_2': tf.TensorSpec(shape= (max_length,), dtype=tf.int32)
        },
    tf.TensorSpec(shape=(vocab_size,),dtype=tf.float32)
    )
  dataset = tf.data.Dataset.from_generator(
        generator,
        output_signature = output_signature
    )
  return dataset.batch(32)

dataset = data_generator(train_descriptions, features,tokenizer,max_length)

dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)
for batch in dataset.take(1):
    inputs, outputs = batch
    print("Input_1 shape:", inputs['input_1'].shape)  # (32, 2048)
    print("Input_2 shape:", inputs['input_2'].shape)  # (32, max_length)
    print("Output shape:", outputs.shape)             # (32, vocab_size)

# dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)
print(dataset)  #making sure it doesnt return None

print(train_descriptions)

def define_model(vocab_size,max_length):
  #CNN model from 2048 nodes to 256 nodes
  inputs1= Input(shape=(2048,), name ='input_1')
  fe1 = Dropout(0.5)(inputs1)
  fe2 = Dense(256, activation = 'relu')(fe1)

  #LSTM sequence model
  inputs2 = Input(shape = (max_length,), name ='input_2')
  se1 = Embedding(vocab_size ,256 , mask_zero = True)(inputs2)
  se2 = Dropout(0.5)(se1)
  se3 = LSTM(256)(se2)

  decoder1 = add([fe2, se3])
  decoder2 = Dense(256, activation='relu')(decoder1)
  outputs = Dense(vocab_size,activation = 'softmax')(decoder2)
  model = Model(inputs = [inputs1 , inputs2], outputs = outputs)

  model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam')
  print(model.summary())
  return model

model = define_model(vocab_size,max_length)
epochs = 25
steps_per_epoch = 5

for i in range(epochs):
  dataset = data_generator(train_descriptions , train_features , tokenizer , max_length)
  model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1)
  model.save("models/model_"+ str(i)+".h5")

import os
import shutil
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define save directory
save_dir = '/content/drive/MyDrive/ImageCaptioning/models'
os.makedirs(save_dir, exist_ok=True)

# Move existing models
for i in range(epochs):  # Assuming epochs=10 from previous run
    local_path = f"models/model_{i}.h5"
    drive_path = os.path.join(save_dir, f"model_{i}.h5")
    if os.path.exists(local_path):
        shutil.move(local_path, drive_path)
        print(f"Moved model to {drive_path}")
    else:
        print(f"Local model {local_path} not found")

# def data_generator(descriptions, features, tokenizer, max_length, vocab_size):
#     def gen():
#         while True:
#             for key, desc_list in descriptions.items():
#                 photo = features[key][0]
#                 for desc in desc_list:
#                     seq = tokenizer.texts_to_sequences([desc])[0]
#                     for i in range(1, len(seq)):
#                         in_seq, out_seq = seq[:i], seq[i]
#                         in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
#                         out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
#                         yield ({'input_1': photo, 'input_2': in_seq}, out_seq)

#     return tf.data.Dataset.from_generator(
#         gen,
#         output_signature=(
#             {
#                 'input_1': tf.TensorSpec(shape=(2048,), dtype=tf.float32),
#                 'input_2': tf.TensorSpec(shape=(max_length,), dtype=tf.int32)
#             },
#             tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)
#         )
#     ).batch(32).prefetch(tf.data.AUTOTUNE)

#Test The model

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.applications.xception import Xception
from keras.models import load_model
from pickle import load
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import argparse
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout
from tensorflow.keras.layers import add
from tensorflow.keras.models import Model

# Instead of argparse
from google.colab import files
uploaded = files.upload()

# Get the filename
img_path = list(uploaded.keys())[0]
 # change this to your image path

def extract_features(filename, model):
        try:
            image = Image.open(filename)

        except:
            print("ERROR: Couldn't open image! Make sure the image path and extension is correct")
        image = image.resize((299,299))
        image = np.array(image)
        # for images that has 4 channels, we convert them into 3 channels
        if image.shape[2] == 4:
            image = image[..., :3]
        image = np.expand_dims(image, axis=0)
        image = image/127.5
        image = image - 1.0
        feature = model.predict(image)
        return feature

def word_for_id(integer, tokenizer):
 for word, index in tokenizer.word_index.items():
     if index == integer:
         return word
 return None

def generate_desc(model, tokenizer, photo, max_length):
    in_text = 'start'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        pred = model.predict([photo,sequence], verbose=0)
        pred = np.argmax(pred)
        word = word_for_id(pred, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'end':
            break
    return in_text

from keras.utils import plot_model

def define_model(vocab_size, max_length):

    # features from the CNN model squeezed from 2048 to 256 nodes
    inputs1 = Input(shape=(2048,), name='input_1')
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    # LSTM sequence model
    inputs2 = Input(shape=(max_length,), name='input_2')
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    # Merging both models
    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    # tie it together [image, seq] [word]
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # summarize model
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)

    return model

max_length = 32
tokenizer = load(open("tokenizer.p","rb"))
vocab_size = len(tokenizer.word_index) + 1

# # First define the model architecture
model = define_model(vocab_size, max_length)
# # Then load the weights
model.load_weights('/content/drive/MyDrive/ImageCaptioning/models/model_9.h5')

# model_path = '/content/drive/MyDrive/ImageCaptioning/models/model_9.h5'
# if os.path.exists(model_path):
#     model = load_model(model_path)
#     print(f"Loaded model from {model_path}")
# else:
#     print(f"Model not found at {model_path}. Redefining and training required.")
#     model = define_model(vocab_size, max_length)
xception_model = Xception(include_top=False, pooling="avg")

photo = extract_features(img_path, xception_model)
img = Image.open(img_path)

description = generate_desc(model, tokenizer, photo, max_length)
print("\n\n")
print(description)
plt.imshow(img)

